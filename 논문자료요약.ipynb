{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markr.AI Data Generation\n",
    "\n",
    "`@ Developer : DopeorNope (Seungyoo Lee), Kyujinpy (Kyunjin Han)`\n",
    "\n",
    "`@ Updata Date: 2024.03.22`\n",
    "\n",
    "`@ Description :` \n",
    "\n",
    "- AI Hub 데이터와, Public Data를 통해서 Fine-tuning용 데이터를 생성하는 코드입니다.\n",
    "    \n",
    "    - 데이터를 생성하기 위해서는 기본 경로에,  '도서자료 요약', '문서요약 텍스트', '018.논문자료 요약 데이터', '048.일반상식 문장 생성 데이터', 'KorQuAD_v1.0_train.json' 데이터를 넣어주시면 됩니다.\n",
    "\n",
    "    - Chat GPT가 아닌, License에 제약이 없는 방법론으로 기존의 데이터들을 생성해내는 것을 학습 세트로 하여 `Custom LLM을 개발` 할 수 있는 것을 목표로 하여 Code를 공유하기로 하였습니다.\n",
    "\n",
    "    - 어떤 말뭉치 데이터(corpus)로도 코드르 조금만 수정하면 원하시는 도메인에 맞는 데이터셋을 생성 할 수 있습니다.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 눈문자료요약 Corpus로 데이터 생성하기\n",
    "\n",
    "- `KoCommercial-Dataset.ipynb`에서 생성한 논문자료요약 Corpus를 통해서 fine-tuning 데이터셋을 생성하는 코드입니다.\n",
    "\n",
    "\n",
    "- 위의 ipynb파일의 파이프라인을 실행 후 사용하시길 권장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import dataset_dict, load_dataset,Dataset,DatasetDict\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=pd.read_parquet('./13/논문자료요약.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = new_df#[new_df['domain'].isin(domains)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus 필터링 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(filtered_df)):\n",
    "    \n",
    "    filtered_df['text'][i] = filtered_df['text'][i].strip()\n",
    "    \n",
    "    try:\n",
    "        if filtered_df['text'][i].endswith('. '):\n",
    "            filtered_df['text'][i]=filtered_df['text'][i][:-1]\n",
    "\n",
    "        elif filtered_df['text'][i].endswith('.'):\n",
    "            pass\n",
    "\n",
    "        elif filtered_df['text'][i].endswith(' .'):\n",
    "            print(i, '번째 문장')\n",
    "            print(filtered_df['text'][i])\n",
    "            filtered_df['text'][i]=filtered_df['text'][i][:-2]+'.'\n",
    "\n",
    "            print(filtered_df['text'][i])\n",
    "            \n",
    "\n",
    "        else:\n",
    "            print(i, '번째 문장')\n",
    "            print(filtered_df['text'][i])\n",
    "\n",
    "\n",
    "            if filtered_df['text'][i].endswith(' '):\n",
    "                filtered_df['text'][i]=filtered_df['text'][i][:-1]+'.'\n",
    "                \n",
    "\n",
    "            else:\n",
    "                filtered_df['text'][i]=filtered_df['text'][i]+'.'\n",
    "                \n",
    "            \n",
    "            print(filtered_df['text'][i])\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(i)\n",
    "        print(filtered_df['text'][i])\n",
    "\n",
    "        print('######################################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_index = []\n",
    "for i in range(len(filtered_df)):\n",
    "    text = filtered_df['text'][i]\n",
    "    \n",
    "    if '�' in text:\n",
    "        remove_index.append(i)\n",
    "    elif '삭제.' in text[-5:]:\n",
    "        remove_index.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_df['text'][remove_index[6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.drop(remove_index, inplace=True)\n",
    "filtered_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df= filtered_df.dropna(subset=['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df=filtered_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_element(text):\n",
    "    return len(text.split('. '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['len_elements'] = filtered_df['text'].apply(lambda x: len(x.split('. ')) if isinstance(x, str) else 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_1 = filtered_df[(filtered_df['len_elements'] > 2)&(filtered_df['len_elements'] < 15)] # 15기준\n",
    "filtered_df_2 = filtered_df[filtered_df['len_elements'] <= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_1=filtered_df_1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_2=filtered_df_2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Sentence order inference\n",
    "\n",
    "- make_text_allign_data 함수는 Sentence order inference를 진행하는 함수입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_text_allign_data(text):\n",
    "\n",
    "    if not (text.endswith('.') or text.endswith('. ')):\n",
    "        #print(text)\n",
    "\n",
    "        text+='.'\n",
    "\n",
    "\n",
    "\n",
    "    if text.endswith('.'):\n",
    "        text+=' '\n",
    "\n",
    "\n",
    "\n",
    "    my_list = text.split('. ')\n",
    "    my_list_with_dots = [element + '.' for element in my_list]\n",
    "\n",
    "    if my_list_with_dots[-1]=='.':\n",
    "\n",
    "        my_list_with_dots=my_list_with_dots[:-1]\n",
    "\n",
    "\n",
    "\n",
    "    random.shuffle(my_list_with_dots)\n",
    "\n",
    "\n",
    "    if len(my_list_with_dots)==1:\n",
    "        raise Exception(\"allign 리스트의 길이가 1입니다.\",text)\n",
    "\n",
    "\n",
    "    tmp_input='당신은 인공지능 비서입니다. 주어진 문장 리스트를 모두 활용하여 가장 정확한 단락을 생성하세요.'\n",
    "    tmp_inst=f'이 문장들은 임의의 순서로 섞여 있습니다. 모든 문장을 활용하여 원본 단락의 순서와 내용을 올바른 순서로 재구성하세요.\\n#문장 리스트: {my_list_with_dots}'\n",
    "    \n",
    "    \n",
    "    if text.endswith(' '):\n",
    "        tmp_out=text[:-1]\n",
    "\n",
    "    else:\n",
    "        tmp_out=text\n",
    "\n",
    "        \n",
    "\n",
    "    return tmp_input, tmp_inst, tmp_out\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Last sentence prediction\n",
    "\n",
    "-  `make_completion_data` 함수는 Last sentence prediction를 진행하는 함수입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_completion_data(text):\n",
    "\n",
    "\n",
    "    if not (text.endswith('.') or text.endswith('. ')):\n",
    "        #print(text)\n",
    "\n",
    "        text+='.'\n",
    "\n",
    "\n",
    "\n",
    "    if text.endswith('. '):\n",
    "        text=text\n",
    "\n",
    "    else:\n",
    "        text=text+' '\n",
    "\n",
    "\n",
    "\n",
    "    my_list_with_dots = text.split('. ')\n",
    "\n",
    "\n",
    "#    print(my_list_with_dots)\n",
    "\n",
    "\n",
    "    # ㅁㅁㅁㅁㅁ. 이렇게 공백있이 끝난경우\n",
    "    if my_list_with_dots[-1]=='':\n",
    "\n",
    "        my_list_with_dots=my_list_with_dots[:-1]\n",
    "\n",
    "        if len(my_list_with_dots)==1:\n",
    "            raise Exception(\"Completion 텍스트를 나눈  리스트의 길이가 1입니다.\",text)\n",
    "\n",
    "        last_sentence=my_list_with_dots.pop()\n",
    "        last_sentence+='.'\n",
    "\n",
    "\n",
    "        \n",
    "    remaining_paragraph=''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(len(my_list_with_dots)):\n",
    "        \n",
    "        if i!=len(my_list_with_dots)-1:\n",
    "            remaining_paragraph+=my_list_with_dots[i]+'. '\n",
    "\n",
    "        else:\n",
    "            remaining_paragraph+=my_list_with_dots[i]+'.'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    tmp_input='당신은 인공지능 비서입니다. 주어진 원문을 바탕으로 주어진 질문에 가장 적절한 답변을 생성하세요.'\n",
    "\n",
    "    tmp_instruct=f'다음 텍스트에서 제공된 문맥을 정확히 이해하고, 마지막 문장을 자연스럽고 문맥에 맞게 완성하세요. 문장은 이전 내용과 논리적으로 연결되어야 합니다.\\n#텍스트: {remaining_paragraph}'\n",
    "\n",
    "    tmp_output=last_sentence\n",
    "\n",
    "\n",
    "    return tmp_input, tmp_instruct, tmp_output\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Mask Prediction\n",
    "\n",
    "-  `make_text_mask_data` 함수는 Mask Prediction를 진행하는 함수입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_text_mask_data(text):\n",
    "\n",
    "\n",
    "    if not (text.endswith('.') or text.endswith('. ')):\n",
    "        #print(text)\n",
    "\n",
    "        text+='.'\n",
    "\n",
    "    if text.endswith('. '):\n",
    "        text=text[:-1]\n",
    "\n",
    "    else:\n",
    "        text=text\n",
    "\n",
    "\n",
    "    words = re.findall(r'[가-힣]{2,}', text)\n",
    "    \n",
    "    random_word = random.choice(words)\n",
    "\n",
    "    \n",
    "\n",
    "  \n",
    "\n",
    "    masked_text = text.replace(random_word, \"<MASK>\")\n",
    "\n",
    "\n",
    "    tmp_input='당신은 인공지능 비서입니다. 주어진 질문에 가장 적절한 답변을 제공하세요.'\n",
    "    tmp_instruct=f'이 문제에서는 주어진 텍스트 내의 <MASK>로 표시된 부분에 들어갈 적절한 단어를 예측해야 합니다. <MASK>가 위치한 문장의 전체 문맥을 분석하여, 문장의 나머지 내용과 일관되게 <MASK>에 들어갈 가장 적합한 단어를 답하세요.\\n#텍스트: {masked_text}'\n",
    "    tmp_output=random_word\n",
    "\n",
    "\n",
    "\n",
    "    return tmp_input, tmp_instruct, tmp_output\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Sentence order inference\n",
    "\n",
    "- `make_word_align` 함수는 Sentence order inference를 진행하는 함수입니다.\n",
    "\n",
    "- 다만 문장 단위가 아닌 `단어 단위`로 생성하는 함수입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_word_align(text):\n",
    "\n",
    "    word_lst=[]\n",
    "    for word in text.split(' '):\n",
    "        out = re.sub(r\"[^\\w\\s]\", \"\", word)\n",
    "        word_lst.append(out)\n",
    "\n",
    "\n",
    "    # 중복 제거하기 위해 set으로 만듬\n",
    "    word_lst=set(word_lst)\n",
    "\n",
    "\n",
    "    # 다시 리스트 형식으로 돌림\n",
    "    word_lst=list(word_lst)\n",
    "\n",
    "\n",
    "    # 랜덤하게 재배열\n",
    "    random.shuffle(word_lst)\n",
    "\n",
    "\n",
    "    tmp_input= '당신은 인공지능 비서입니다. 주어진 지시사항에 따라 가장 적절한 문장을 생성하세요.'\n",
    "    tmp_instruct=f'이 문제에는 문장에서 공백을 기준으로 나누고, 구두점을 제거한 무작위로 섞인 단어들이 담긴 리스트가 제공됩니다. 이 리스트의 단어를 모두 활용하여 가장 문맥상 적절한 문장을 생성하세요.\\n#단어리스트: {word_lst}'\n",
    "    tmp_output=text\n",
    "\n",
    "    return tmp_input, tmp_instruct, tmp_output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 생성 파이프라인\n",
    "\n",
    "- 여기서는 `두번의 파이프라인`을 거쳐서 진행되게 됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lst=[]\n",
    "output_lst=[]\n",
    "inst_lst=[]\n",
    "id_lst=[]\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(filtered_df_1))):\n",
    "    try:\n",
    "\n",
    "        text=filtered_df_1['text'][i]\n",
    "        \n",
    "\n",
    "        thred=random.random()\n",
    "\n",
    "\n",
    "        if thred < 0.44 :\n",
    "\n",
    "            tmp_input, tmp_instruct, tmp_output = make_completion_data(text)\n",
    "            tmp_id='completion_aihub'\n",
    "\n",
    "        elif thred < 0.88:\n",
    "            tmp_input, tmp_instruct, tmp_output = make_text_allign_data(text)\n",
    "            tmp_id='text_allign_aihub'\n",
    "      \n",
    "\n",
    "        else:\n",
    "            \n",
    "            tmp_input, tmp_instruct, tmp_output = make_text_mask_data(text)\n",
    "            tmp_id='pre_mask_aihub'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        input_lst.append(tmp_input)\n",
    "        inst_lst.append(tmp_instruct)\n",
    "        output_lst.append(tmp_output)\n",
    "        id_lst.append(tmp_id)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"{i}번째 행\")\n",
    "        print(e)\n",
    "        print(filtered_df_2['text'][i])\n",
    "        print(\"----------------------------------------\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(inst_lst)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output_lst)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(id_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_df_1=pd.DataFrame({'input':input_lst,'instruction':inst_lst,'output':output_lst})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lst=[]\n",
    "output_lst=[]\n",
    "inst_lst=[]\n",
    "id_lst=[]\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(filtered_df_2))):\n",
    "    try:\n",
    "\n",
    "        text=filtered_df_2['text'][i]\n",
    "\n",
    "        \n",
    "\n",
    "        thred=random.random()\n",
    "\n",
    "        if thred < 0.56 :\n",
    "\n",
    "            tmp_input, tmp_instruct, tmp_output = make_word_align(text)\n",
    "            tmp_id='word_allign_aihub'\n",
    "\n",
    "        else:\n",
    "            \n",
    "            tmp_input, tmp_instruct, tmp_output = make_text_mask_data(text)\n",
    "            tmp_id='pre_mask_aihub'\n",
    "\n",
    "\n",
    "        input_lst.append(tmp_input)\n",
    "        inst_lst.append(tmp_instruct)\n",
    "        output_lst.append(tmp_output)\n",
    "        id_lst.append(tmp_id)\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"{i}번째 행\")\n",
    "        print(e)\n",
    "        print(filtered_df_2['text'][i])\n",
    "        print(\"----------------------------------------\")\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_df_2=pd.DataFrame({'input':input_lst,'instruction':inst_lst,'output':output_lst})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df=pd.concat([hub_df_1,hub_df_2],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.to_parquet('./Save/논문자료요약.parquet', engine = 'pyarrow', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aimv",
   "language": "python",
   "name": "aimv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
